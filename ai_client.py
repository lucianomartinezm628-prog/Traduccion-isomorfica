import streamlit as st
import google.generativeai as genai
import json
import os
import time
import random
from google.api_core import exceptions

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 1. CONFIGURACI√ìN DE MODELOS DISPONIBLES
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
MODELOS_DISPONIBLES = {
    "flash_2.0": "gemini-2.0-flash",       
    "flash_lite": "gemini-flash-lite-latest", 
    "pro_latest": "gemini-pro-latest",     
}

# Selecci√≥n del modelo (puedes cambiar la clave seg√∫n prefieras)
MODELO_ACTUAL = MODELOS_DISPONIBLES["flash_lite"]

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 2. AUTENTICACI√ìN SEGURA (Streamlit Secrets / Env)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if "GOOGLE_API_KEY" in st.secrets:
    api_key = st.secrets["GOOGLE_API_KEY"]
else:
    api_key = os.getenv("GOOGLE_API_KEY")

if api_key:
    genai.configure(api_key=api_key)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 3. CLIENTE GEMINI CON GESTI√ìN DE TR√ÅFICO
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

class GeminiClient:
    def __init__(self):
        if not api_key:
            st.error("‚ö†Ô∏è No se detect√≥ la GOOGLE_API_KEY. Config√∫rala en los Secrets de Streamlit.")
            self.model = None 
            return

        try:
            self.model = genai.GenerativeModel(
                MODELO_ACTUAL, 
                generation_config={"response_mime_type": "application/json"}
            )
            print(f"[*] Sistema conectado al modelo: {MODELO_ACTUAL}")
        except Exception as e:
            st.error(f"Error al inicializar el modelo {MODELO_ACTUAL}: {e}")
            self.model = None

    def _generar_con_retry(self, prompt: str, intentos_max: int = 5) -> list:
        """
        Maneja el error 429 (Cuota excedida) aplicando una espera
        exponencial antes de reintentar la consulta.
        """
        if not self.model: 
            return []

        for i in range(intentos_max):
            try:
                response = self.model.generate_content(prompt)
                return json.loads(response.text)
            
            except exceptions.ResourceExhausted:
                # Error 429: Demasiado tr√°fico. 
                # Espera base de 5s + aumento exponencial (2^i) + azar
                tiempo_espera = 5 + (2 ** i) + random.uniform(0, 2)
                
                msg = f"üö¶ Alto tr√°fico en {MODELO_ACTUAL}. Esperando {int(tiempo_espera)}s..."
                print(msg)
                try:
                    st.toast(msg, icon="‚è≥")
                except:
                    pass
                
                time.sleep(tiempo_espera)
                continue # Reintentar
                
            except Exception as e:
                print(f"‚ùå Error en consulta IA: {e}")
                return []
        
        print("‚ùå Se agotaron los reintentos para este token.")
        return []

    def consultar_nucleo(self, token: str, contexto: str) -> list:
        """Protocolo P4: An√°lisis etimol√≥gico de n√∫cleos l√©xicos"""
        prompt = f"""
        Act√∫a como fil√≥logo experto siguiendo el protocolo P4.
        Analiza el token: '{token}'
        Contexto: '{contexto}'
        
        Responde exclusivamente en formato JSON con una LISTA de objetos: 
        [{{
            "termino": "traducci√≥n_espa√±ola",
            "origen": "LATINA" | "GRIEGA" | "ARABE" | "TECNICA",
            "raiz": "raiz_etimol√≥gica",
            "derivacion_existe": bool,
            "es_metafora_viable": bool
        }}]
        """
        return self._generar_con_retry(prompt)

    def consultar_particula(self, token: str, funcion_sintactica: str) -> list:
        """Protocolo P5: An√°lisis funcional de part√≠culas"""
        prompt = f"""
        Act√∫a como experto gramatical siguiendo el protocolo P5.
        Analiza la part√≠cula: '{token}'
        Funci√≥n: '{funcion_sintactica}'
        
        Responde exclusivamente en formato JSON con una LISTA de objetos: 
        [{{
            "termino": "traducci√≥n",
            "es_etimologico": bool,
            "cierra_regimen": bool
        }}]
        """
        return self._generar_con_retry(prompt)

# Instancia global para el orquestador
ai_engine = GeminiClient()
